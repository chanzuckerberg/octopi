{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import zarr, copick\n",
    "from tqdm import tqdm\n",
    "from monai.data import DataLoader, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    ScaleIntensityRanged, \n",
    "    CropForegroundd, \n",
    "    Orientationd, \n",
    "    Spacingd, \n",
    "    EnsureTyped, \n",
    "    Activations, \n",
    "    AsDiscrete, \n",
    "    Resized, \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    RandZoomd,\n",
    "    RandGridPatchd,\n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    "    Resized, \n",
    "    RandZoomd,\n",
    "    Activations, \n",
    "    CropForegroundd, \n",
    "    ScaleIntensityRanged, \n",
    "    RandCropByPosNegLabeld,    \n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss, FocalLoss, TverskyLoss\n",
    "from monai.metrics import DiceMetric, ConfusionMatrixMetric\n",
    "# from skimage\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, NormalizeIntensityd, Orientationd, RandCropByLabelClassesd, RandRotate90d, RandFlipd\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "import mlflow, optuna, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "copick_config_path = \"../../../simulations/ml_challenge/ml_config.json\"\n",
    "root = copick.from_file(copick_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tomogram_array(copick_run, voxel_spacing=10, tomo_type='wbp'):\n",
    "    voxel_spacing_obj = copick_run.get_voxel_spacing(voxel_spacing)\n",
    "    tomogram = voxel_spacing_obj.get_tomogram(tomo_type)\n",
    "    image = zarr.open(tomogram.zarr(), mode='r')['0']\n",
    "    return image[:]\n",
    "\n",
    "def get_segmentation_array(copick_run, segmentation_name, voxel_spacing=10, is_multilabel=True):\n",
    "    #seg_memb = copick_run.get_segmentations(name=\"membrane\")\n",
    "    seg_memb = copick_run.get_segmentations()\n",
    "    seg = copick_run.get_segmentations(is_multilabel=is_multilabel, name=segmentation_name, voxel_size=voxel_spacing)\n",
    "    if len(seg) == 0:\n",
    "        raise ValueError(f\"No segmentations found for session '{session_id}' and segmentation type '{segmentation_type}'.\")\n",
    "        \n",
    "    segmentation = zarr.open(seg[0].zarr().path, mode=\"r\")['0'][:]\n",
    "    _, array = list(zarr.open(seg_memb[0].zarr()).arrays())[0]\n",
    "    seg_membrane = np.array(array[:])\n",
    "    #seg_membrane = zarr.open(seg_memb[0].zarr().path, mode=\"r\")['0'][:]\n",
    "    segmentation[seg_membrane==1] = 1\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:11<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "voxel_spacing = 10\n",
    "tomo_type = \"wbp\"\n",
    "painting_segmentation_name = \"segmentation\" #\"remotetargets\"\n",
    "\n",
    "runIDs = [run.name for run in root.runs]\n",
    "data_dicts = []\n",
    "for runID in tqdm(runIDs):\n",
    "    run = root.get_run(str(runID))\n",
    "    tomogram = get_tomogram_array(run)\n",
    "    segmentation = get_segmentation_array(run, painting_segmentation_name)\n",
    "    data_dicts.append({\"image\": tomogram, \"label\": segmentation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 15\n",
      "Number of validation samples: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 15/15 [00:00<00:00, 20.47it/s]\n",
      "Loading dataset: 100%|██████████| 7/7 [00:00<00:00, 23.96it/s]\n"
     ]
    }
   ],
   "source": [
    "my_num_samples = 16\n",
    "train_batch_size = 1\n",
    "val_batch_size = 1\n",
    "\n",
    "test_files = data_dicts[-2:]\n",
    "train_files, val_files = train_test_split(data_dicts[:-2], test_size=0.3)\n",
    "print(f\"Number of training samples: {len(train_files)}\")\n",
    "print(f\"Number of validation samples: {len(val_files)}\")\n",
    "\n",
    "# Non-random transforms to be cached\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")\n",
    "])\n",
    "\n",
    "# Random transforms to be applied during training\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[96, 96, 96],\n",
    "        num_classes=8,\n",
    "        num_samples=my_num_samples\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),    \n",
    "])\n",
    "\n",
    "# Create the cached dataset with non-random transforms\n",
    "train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "# Wrap the cached dataset to apply random transforms during iteration\n",
    "train_ds = Dataset(data=train_ds, transform=random_transforms)\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Validation transforms\n",
    "val_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[96, 96, 96],\n",
    "        num_classes=8,\n",
    "        num_samples=my_num_samples,  # Use 1 to get a single, consistent crop per image\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Create validation dataset\n",
    "val_ds = CacheDataset(data=val_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "# Wrap the cached dataset to apply random transforms during iteration\n",
    "val_ds = Dataset(data=val_ds, transform=random_transforms)\n",
    "\n",
    "# Create validation DataLoader\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    shuffle=False,  # Ensure the data order remains consistent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = DiceLoss(include_background=True, to_onehot_y=True, softmax=True)  # softmax=True for multiclass\n",
    "loss_function = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)  \n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)  # must use onehot for multiclass\n",
    "recall_metric = ConfusionMatrixMetric(include_background=False, metric_name=\"recall\", reduction=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_patches(data):\n",
    "    shape = data.shape\n",
    "    new_shape = (shape[0] * shape[1],) + shape[2:]\n",
    "    return data.view(new_shape)\n",
    "\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=len(root.pickable_objects)+1)\n",
    "post_label = AsDiscrete(to_onehot=len(root.pickable_objects)+1)\n",
    "\n",
    "def train(train_loader, \n",
    "        val_loader, \n",
    "        model, \n",
    "        device,\n",
    "        loss_function, \n",
    "        metrics_function, \n",
    "        optimizer, \n",
    "        max_epochs=100,\n",
    "        val_interval = 2):\n",
    "\n",
    "    best_f1_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "\n",
    "    # Initialize tqdm around the epoch loop\n",
    "    for epoch in tqdm(range(max_epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = batch_data[\"image\"].to(device)  # Shape: [B, C, H, W, D]\n",
    "            labels = batch_data[\"label\"].to(device)  # Shape: [B, C, H, W, D]            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)    # Output shape: [B, num_classes, H, W, D]\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "            avg_loss = epoch_loss / step\n",
    "        \n",
    "        # Compute and log average epoch loss\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch+1)\n",
    "\n",
    "        # Update tqdm description with dynamic loss for each epoch\n",
    "        tqdm.write(f\"Epoch {epoch + 1}/{max_epochs}, avg_train_loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs = val_data[\"image\"].to(device)\n",
    "                    val_labels = val_data[\"label\"].to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    \n",
    "                    # Decollate batches into lists\n",
    "                    val_outputs_list = decollate_batch(val_outputs)\n",
    "                    val_labels_list = decollate_batch(val_labels)\n",
    "                    \n",
    "                    # Apply post-processing\n",
    "                    metric_val_outputs = [post_pred(i) for i in val_outputs_list]\n",
    "                    metric_val_labels = [post_label(i) for i in val_labels_list]\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    metrics_function(y_pred=metric_val_outputs, y=metric_val_labels)\n",
    "\n",
    "                # Contains recall, precision, and f1 for each class\n",
    "                metric_values = metrics_function.aggregate(reduction='mean_batch')\n",
    "\n",
    "                # Log per-class metrics\n",
    "                for i, (rec, prec, f1) in enumerate(zip(metric_values[0], metric_values[1], metric_values[2])):\n",
    "                    mlflow.log_metric(f\"recall_class_{i+1}\", rec.item(), step=epoch + 1)\n",
    "                    mlflow.log_metric(f\"precision_class_{i+1}\", prec.item(), step=epoch + 1)\n",
    "                    mlflow.log_metric(f\"f1_score_class_{i+1}\", f1.item(), step=epoch + 1)\n",
    "\n",
    "                # Log average metrics across all classes\n",
    "                avg_recall =  metric_values[0].mean()\n",
    "                avg_precision =  metric_values[1].mean()\n",
    "                avg_f1 =  metric_values[2].mean()                                \n",
    "\n",
    "                mlflow.log_metric(\"avg_recall\", avg_recall, step=epoch + 1)\n",
    "                mlflow.log_metric(\"avg_precision\", avg_precision, step=epoch + 1)\n",
    "                mlflow.log_metric(\"avg_f1_score\", avg_f1, step=epoch + 1)\n",
    "\n",
    "                tqdm.write(f\"Epoch {epoch + 1}/{max_epochs}, avg_recall: {avg_recall:.4f}\")        \n",
    "                metrics_function.reset()\n",
    "\n",
    "                # Track the best F1 score (mean across all classes)\n",
    "                if avg_f1 > best_f1_metric:\n",
    "                    best_f1_metric = avg_f1\n",
    "\n",
    "                # metric_values.append(metric)\n",
    "                # if metric > best_metric:\n",
    "                #     best_metric = metric\n",
    "                #     best_metric_epoch = epoch + 1\n",
    "                #     torch.save(model.state_dict(), os.path.join('./', \"best_metric_model.pth\"))\n",
    "                    \n",
    "                #     print(\"saved new best metric model\")\n",
    "                # print(\n",
    "                #     f\"current epoch: {epoch + 1} current mean recall per class: {', '.join(metric_per_class)}\"\n",
    "                #     f\"\\nbest mean recall: {best_metric:.4f}\"\n",
    "                #     f\"at epoch: {best_metric_epoch}\"\n",
    "                # )\n",
    "    return best_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set the seed for Python's random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set the seed for PyTorch (both CPU and GPU)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n",
    "\n",
    "    # Ensure reproducibility of operations by disabling certain optimizations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,  \n",
    "              n_classes,\n",
    "              train_loader, \n",
    "              val_loader, \n",
    "              loss_function,\n",
    "              metrics_function, \n",
    "              epochs,\n",
    "              random_seed = 42,\n",
    "              gpu_count = 1):\n",
    "\n",
    "    set_seed(random_seed)\n",
    "\n",
    "    # Assign each trial to a specific GPU based on the trial number\n",
    "    if gpu_count > 1:\n",
    "        gpu_id = trial.number % gpu_count  # Cycle through available GPUs\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "        torch.cuda.set_device(device)  # Set the current GPU for this trial\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    # Set a unique run name for each trial\n",
    "    trial_num = f\"trial_{trial.number}\"\n",
    "\n",
    "    # Start a new MLflow run for each trial\n",
    "    with mlflow.start_run(run_name = trial_num, nested=True):  # Nested=True allows it to be part of the overall experiment run\n",
    "        \n",
    "        # Sample number of channels\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 3, 5)\n",
    "        \n",
    "        # Generate increasing channels based on 16, 32, 64\n",
    "        base_channel = trial.suggest_categorical(\"base_channel\", [8, 16, 32, 64])\n",
    "        channels = [base_channel * (2 ** i) for i in range(num_layers)]\n",
    "\n",
    "        # Strides pattern\n",
    "        # Sample number of downsampling layers (those with stride of 2)\n",
    "        num_downsampling_layers = trial.suggest_int(\"num_downsampling_layers\", 1, num_layers - 1)\n",
    "\n",
    "        # Define strides: first num_downsampling_layers with stride 2, and the rest with stride 1\n",
    "        strides_pattern = [2] * num_downsampling_layers + [1] * (num_layers - num_downsampling_layers - 1)        \n",
    "\n",
    "        # Number of residual units\n",
    "        num_res_units = trial.suggest_int(\"num_res_units\", 1, 3)\n",
    "\n",
    "        print('Current Parameters: ')\n",
    "        print(f'Num Layers: {num_layers}')\n",
    "        print(f'Num Res Units: {num_res_units}')\n",
    "        print(f'Channels: {channels}')\n",
    "        print(f'Num Downsampling Layers: {num_downsampling_layers}')\n",
    "        print(f'Strides: {strides_pattern}\\n')\n",
    "\n",
    "        # Log parameters to MLflow at each trial\n",
    "        mlflow.log_params({\n",
    "            \"num_layers\": num_layers,\n",
    "            \"base_channel\": base_channel,\n",
    "            \"channels\": channels,\n",
    "            \"num_downsampling_layers\": num_downsampling_layers,\n",
    "            \"strides_pattern\": strides_pattern,\n",
    "            \"num_res_units\": num_res_units,\n",
    "            \"random_seed\": random_seed\n",
    "        })\n",
    "\n",
    "        # Now use channels, strides, and num_res_units in your model definition\n",
    "        model = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=n_classes,\n",
    "            channels=channels,\n",
    "            strides=strides_pattern,\n",
    "            num_res_units=num_res_units,\n",
    "        ).to(device)\n",
    "\n",
    "        # # Sample learning rate using Optuna\n",
    "        # learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "\n",
    "        # Define your loss and optimizer, and return the objective (e.g., validation loss)\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "        score = train(train_loader, val_loader, \n",
    "                      model, device, loss_function, \n",
    "                      metrics_function, \n",
    "                      optimizer, max_epochs=epochs)\n",
    "\n",
    "        # Log the score (e.g., validation loss or F1 score) for each trial\n",
    "        mlflow.log_metric(\"score\", score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-GPU Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 22:18:19,890] A new study created in memory with name: no-name-c8781863-c0ce-4ae5-975f-a466f438b124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Architecture Search Over 4 GPUS\n",
      "\n",
      "Current Parameters: \n",
      "Num Layers: 5\n",
      "Num Res Units: 1\n",
      "Channels: [32, 64, 128, 256, 512]\n",
      "Num Downsampling Layers: 2\n",
      "Strides: [2, 2, 1, 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/100 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:09<15:43,  9.53s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, avg_train_loss: 0.9320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:19<15:43,  9.53s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, avg_train_loss: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:25<21:28, 13.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, avg_recall: 0.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:30<24:30, 15.01s/epoch]\n",
      "[W 2024-10-17 22:18:50,094] Trial 0 failed with parameters: {'num_layers': 5, 'base_channel': 32, 'num_downsampling_layers': 2, 'num_res_units': 1} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_23820/582407621.py\", line 14, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, nclasses, train_loader, val_loader,\n",
      "  File \"/tmp/ipykernel_23820/3199552931.py\", line 79, in objective\n",
      "    score = train(train_loader, val_loader,\n",
      "  File \"/tmp/ipykernel_23820/3983403781.py\", line 30, in train\n",
      "    for batch_data in train_loader:\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1327, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1283, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1131, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/queue.py\", line 180, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/threading.py\", line 324, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-17 22:18:50,096] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[1;32m     13\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trials\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Run trials on multiple GPUs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[1;32m     13\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     16\u001b[0m                    n_trials\u001b[38;5;241m=\u001b[39mnum_trials) \u001b[38;5;66;03m# Run trials on multiple GPUs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 79\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, n_classes, train_loader, val_loader, loss_function, metrics_function, epochs, random_seed, gpu_count)\u001b[0m\n\u001b[1;32m     76\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     77\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr)\n\u001b[0;32m---> 79\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmetrics_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Log the score (e.g., validation loss or F1 score) for each trial\u001b[39;00m\n\u001b[1;32m     85\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, score)\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, device, loss_function, metrics_function, optimizer, max_epochs, val_interval)\u001b[0m\n\u001b[1;32m     28\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     31\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     32\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Shape: [B, C, H, W, D]\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/mnt/jonathan/2024_czii_mlchallenge_notebooks/pyUNET/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "num_trials = 10\n",
    "nclasses = len(root.pickable_objects)+1\n",
    "loss_function = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)  \n",
    "my_metrics = ConfusionMatrixMetric(include_background=False, metric_name=[\"recall\",'precision','f1 score'], reduction=\"none\")\n",
    "\n",
    "# Get available GPUs (for example, on an 8 GPU node)\n",
    "gpu_count = torch.cuda.device_count()\n",
    "\n",
    "print(f'Running Architecture Search Over {gpu_count} GPUS\\n')\n",
    "mlflow.set_experiment('unet-model-search')\n",
    "with mlflow.start_run():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, nclasses, train_loader, val_loader, \n",
    "                                           loss_function, my_metrics, epochs), \n",
    "                   n_trials=num_trials) # Run trials on multiple GPUs\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "    # Log the best trial in MLflow\n",
    "    mlflow.log_metric(\"best_trial_value\", study.best_trial.value)\n",
    "    mlflow.log_params(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
