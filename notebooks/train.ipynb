{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D CNN Instance Segmentation of Proteins in Cryo-ET Tomograms\n",
    "\n",
    "This tutorial guides you through the process of training 3D U-Nets for instance segmentation of proteins in Cryo-ET tomograms. It draws inspiration from the segmentation framework introduced by E. Moebel and co-authors in DeepFinder. However, this repository introduces new developments in model architectures, data augmentations, and efficient model training, with support for datasets available on both local and remote resources.\n",
    "\n",
    "#### üßê What you'll learn\n",
    "In this notebook, we demonstrate how to use the octopi workflow to predict 3D protein coordinates from segmentation masks. We illustrate this using Dataset ID: [10440](https://cryoetdataportal.czscience.com/datasets/10440) ‚Äî a benchmark dataset provided as part of the CZ Imaging Institute Machine Learning Challenge.\n",
    "\n",
    "This dataset includes six experimental tomgorams annotated with known macromolecular species: \n",
    "\n",
    "* Apoferritin \n",
    "* Beta-amylase \n",
    "* Beta-galactosidase \n",
    "* Ribosome \n",
    "* Thyroglobulin \n",
    "* Virus-like particles (VLP)\n",
    "\n",
    "To learn more about the dataset and challenge, see the full preprint here: üìÑ [A Machine Learning Challenge for the Instance Segmentation of Proteins in Cryo-ET](https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.11.04.621686.full.pdf)\n",
    "\n",
    "### üìö Tutorial Overview\n",
    "The tutorial is structured into two main components:\n",
    "\n",
    "1. Data Preparation: Generating target volumes that the network will use to predict coordinates.\n",
    "2. Model Training: Training the 3D U-Net model.\n",
    "3. Optuna Optimization (Optional): Explore Several Model Configurations with Bayesian Optimization\n",
    "\n",
    "**Note:** Inference is provided in the following notebook: `inference.ipynb`\n",
    "\n",
    "By following this tutorial, you will gain insights into preparing data, training a 3D U-Net model for the instance segmentation of proteins in Cryo-ET tomograms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß± Step 1: Data Preparation: Generate Targets for Training\n",
    "\n",
    "In this step, we will prepare the target data necessary for training our model and predicting the coordinates of proteins within a tomogram.\n",
    "\n",
    "We will use the Copick tool to manage the filesystem, extract tomogram IDs, and create spherical targets corresponding to the locations of proteins. The key tasks performed in this cell include:\n",
    "\n",
    "* **Loading Parameters:** \n",
    "\n",
    "We define the size of the target spheres, specify the copick path, voxel size, target file name, and user ID.\n",
    "* **Generating Targets:**\n",
    "\n",
    "For each tomogram, we extract particle coordinates, reset the target volume, generate spherical targets based on these coordinates, and save the target data in OME Zarr format. The equivalent CLI tool for this step is:\n",
    "```\n",
    "octopi create-targets --help\n",
    "```\n",
    "\n",
    "##### üí° Notes:\n",
    "* **Data Access via [copick](https://github.com/copick/copick):**\n",
    "\n",
    "octopi assumes that tomograms and coordinates are accessible through the copick configuration system.\n",
    "\n",
    "* **Alternative Input Sources:**\n",
    "\n",
    "If your data is stored in a folder as `*.mrc` volumes (e.g., from another processing pipeline), you can import them using:\n",
    "```\n",
    "octopi import-mrc-volumes --help\n",
    "```\n",
    "\n",
    "* **Download from the [Data-Portal](https://cryoetdataportal.czscience.com)**\n",
    "\n",
    "We can also download tomograms from the data-portal to speed up processing by avoiding runtime downloads, you can fetch tomograms in advance:\n",
    "```\n",
    "octopi download-dataportal --help\n",
    "```\n",
    "* **Recommended Resolution:**\n",
    "\n",
    "Tomogarms should ideally be resampled to at least 10 √Ö per voxel. This reduces memory usage and speeds up training without significantly sacrificing performance. When import data from either MRC formats, or downloading directly from the data-portal we can downsample to the desired resolution with the `--output-voxel-size` flag. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopi.entry_points.run_create_targets import create_sub_train_targets, create_all_train_targets\n",
    "\n",
    "# Copick Config\n",
    "config = '../config.json'\n",
    "\n",
    "# Target Parameters\n",
    "target_name = 'targets'\n",
    "target_user_id = 'octopi'               # These parameters are optional\n",
    "target_session_id = '0'\n",
    "\n",
    "# Tomogram Query Information - This is Used to determine the resolution that the targets will be created for. \n",
    "voxel_size = 10.012\n",
    "tomogram_algorithm = 'wbp-denoised-denoiset-ctfdeconv'\n",
    "\n",
    "# For our segmetnation target, we can create a sphere with a diameter that is a fraction of the \n",
    "# particle radius provided in the config file.\n",
    "radius_scale = 0.7\n",
    "\n",
    "# Optional: Define A Sub-set of tomograms for generating training labels\n",
    "run_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the segmentation targets, we can use to optional functions that are available. \n",
    "1. We can provide a subset of pickable objects and (optionally) its userID / sessionIds. This allows for creating training targets from varying submission sources.\n",
    "2. Instead of Manually specifying each individual pick targets by the name (and potentially its sessionID and/or userID). We can find all the pickable objects associated with a single query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Targets for the following objects: ribosome, virus-like-particle, apoferritin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 88 picks in 16463...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 1/7 [00:19<01:11, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 81 picks in 16464...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñä       | 2/7 [00:28<00:52, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 142 picks in 16465...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:37<00:38,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 83 picks in 16466...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:47<00:28,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 163 picks in 16467...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:56<00:19,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 148 picks in 16468...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:58<00:09,  9.61s/it]"
     ]
    }
   ],
   "source": [
    "# Option 1: We can provide a subset of pickable objects and (optionally) its userID / sessionIds. \n",
    "# This allows for creating training targets from varying submission sources.\n",
    "# Provide inputs as a list of tuples -> [ (name, userID, sessionI)]\n",
    "\n",
    "pick_targets = [\n",
    "    ('ribosome', 'data-portal', None),\n",
    "    ('virus-like-particle', 'data-portal', None),\n",
    "    ('apoferritin', 'data-portal', None)\n",
    "]\n",
    "\n",
    "seg_targets = [] # Either provide this variable as an empty list or populate entries in the same format (name, userID, sessionID)\n",
    "\n",
    "create_sub_train_targets(\n",
    "    config, pick_targets, seg_targets, voxel_size, radius_scale, tomogram_algorithm,\n",
    "    target_name, target_user_id, target_session_id, run_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Targets for the following objects: apoferritin, beta-amylase, beta-galactosidase, ribosome, thyroglobulin, virus-like-particle, membrane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 136 picks in 16463...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 1/7 [00:19<01:12, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 141 picks in 16464...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñä       | 2/7 [00:28<00:50, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 191 picks in 16465...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:37<00:38,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 143 picks in 16466...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:46<00:28,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 215 picks in 16467...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:55<00:18,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 221 picks in 16468...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [01:04<00:09,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating 202 picks in 16469...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [01:06<00:00,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of targets complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Instead of Manually Specifying Each pickable object, we can provide a single query \n",
    "# and it will grab the first available coordinate for each pickable object.\n",
    "picks_user_id = 'data-portal'\n",
    "picks_session_id = None\n",
    "\n",
    "# In this case, we don't have any organelle segmentations that are at 10 Angstroms on the portal\n",
    "seg_targets = []\n",
    "\n",
    "create_all_train_targets(\n",
    "    config, seg_targets, picks_session_id, picks_user_id, \n",
    "    voxel_size, radius_scale, tomogram_algorithm, \n",
    "    target_name, target_user_id, target_session_id, run_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Training the octopi üêô to find macromolecules in Cryo-ET Tomograms\n",
    "\n",
    "Once our target labels are prepared, we can begin training a deep learning model to identify macromolecular structures in our data. \n",
    "\n",
    "The training process is modular and configurable. It involves defining a target segmentation volumes (prepared in Step 1), preparing 3D tomographic input data, and configuring a U-Net-based segmentation model to predict voxel-level class assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import ConfusionMatrixMetric\n",
    "from octopi.models import common as builder\n",
    "from octopi.datasets import generators\n",
    "from monai.losses import TverskyLoss\n",
    "from octopi import losses\n",
    "from octopi.pytorch import trainer \n",
    "from octopi import io, utils\n",
    "import torch, os\n",
    "\n",
    "########### Input Parameters ###########\n",
    "\n",
    "# Target Parameters\n",
    "config = \"../config.json\"\n",
    "target_name = 'targets'\n",
    "target_user_id = 'octopi'\n",
    "target_session_id = None\n",
    "\n",
    "# DataGenerator Parameters\n",
    "num_tomo_crops = 16\n",
    "tomo_algorithm = 'wbp-denoised-denoiset-ctfdeconv'\n",
    "voxel_size = 10.012\n",
    "# In cases where all the tomograms can't be fit in memory, we can train on smaller batches\n",
    "tomo_batch_size = 25\n",
    "\n",
    "# Model Parameters\n",
    "Nclass = 7\n",
    "model_config = {\n",
    "        'architecture': 'Unet',            # Model Architecture\n",
    "        'channels': [32,64,128,128],   # Number of Channels in Each Layer \n",
    "        'strides': [2, 2, 1, 1],        # Strides for the convolutional layers\n",
    "        'num_res_units': 3,                # Number of Residual units\n",
    "        'num_classes': Nclass,                  # Number of Classes on prediction head (background + numClasses)\n",
    "        'dropout': 0.05,                    # Drop Out\n",
    "        'dim_in': 128                      # Input Dimensions [voxels]\n",
    "    }\n",
    "\n",
    "model_save_path = 'results'         # Path to save the model\n",
    "model_weights = None # Path to the pre-trained model weights\n",
    "\n",
    "# Optional - Specify RunIDs for training and validation data splits. \n",
    "trainRunIDs = None\n",
    "validateRunIDs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Prepare the training module\n",
    "\n",
    "Next, we instantiate the octopi data generator, which handles on-the-fly loading of sub-volumes from the full tomograms. This is especially helpful when training on large datasets that cannot fit into memory.\n",
    "\n",
    "We also define the custom loss and metric functions. Here we use a Weighted Focal Tversky Loss, which is well-suited for class-imbalanced volumetric data, and a multi-class confusion matrix metric to compute recall, precision, and F1 score per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6\n",
      "Number of validation samples: 1\n",
      "Number of test samples: 0\n",
      "All training samples fit in memory. No reloading required.\n"
     ]
    }
   ],
   "source": [
    "# Single-config training\n",
    "data_generator = generators.TrainLoaderManager(\n",
    "    config, \n",
    "    target_name, \n",
    "    target_session_id = target_session_id,\n",
    "    target_user_id = target_user_id,\n",
    "    tomo_algorithm = tomo_algorithm,\n",
    "    voxel_size = voxel_size,\n",
    "    Nclasses = Nclass,\n",
    "    tomo_batch_size = tomo_batch_size )\n",
    "\n",
    "# Get the data splits\n",
    "data_generator.get_data_splits(trainRunIDs = trainRunIDs,\n",
    "                                validateRunIDs = validateRunIDs,\n",
    "                                train_ratio = 0.9, val_ratio = 0.1, test_ratio = 0.0,\n",
    "                                create_test_dataset=False)\n",
    "\n",
    "# Get the reload frequency\n",
    "data_generator.get_reload_frequency(num_epochs)\n",
    "\n",
    "# Monai Functions\n",
    "alpha0 = 0.1\n",
    "gamma0 = 1.88\n",
    "weight = 0.13\n",
    "loss_function = losses.WeightedFocalTverskyLoss(\n",
    "    gamma=gamma0, alpha = alpha0, beta = (1-alpha0),\n",
    "    weight_tversky = weight, weight_focal = (1-weight)\n",
    ")\n",
    "metrics_function = ConfusionMatrixMetric(include_background=False, metric_name=[\"recall\",'precision','f1 score'], reduction=\"none\")\n",
    "\n",
    "# Create UNet Model and Load Weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_builder = builder.get_model(model_config['architecture'])\n",
    "model = model_builder.build_model(model_config)\n",
    "if model_weights: \n",
    "    model.load_state_dict(torch.load(model_weights, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-3   # Learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr, weight_decay=0.1)\n",
    "\n",
    "# Create UNet-Trainer\n",
    "model_trainer = trainer.ModelTrainer(model, device, loss_function, metrics_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãÔ∏è Train the model\n",
    "\n",
    "Finally, we initiate model training for a user-defined number of epochs. Validation is run at regular intervals (val_interval), and the best-performing model is tracked based on a specified metric (avg_fBeta by default).\n",
    "\n",
    "Training results and metadata are saved to disk at the end for future analysis and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_fBeta is not a valid metric! Tracking avg_f1 as the best metric\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.43it/s]\n",
      "Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Training Progress:   0%|          | 4/1000 [00:28<1:34:53,  5.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000, avg_train_loss: 0.9194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 4/1000 [00:30<1:34:53,  5.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000, avg_f1_score: 0.0123, avg_recall: 0.2163, avg_precision: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 9/1000 [00:59<1:36:37,  5.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, avg_train_loss: 0.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 9/1000 [01:01<1:36:37,  5.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, avg_f1_score: 0.0209, avg_recall: 0.2545, avg_precision: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|‚ñè         | 14/1000 [01:29<1:36:25,  5.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000, avg_train_loss: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|‚ñè         | 14/1000 [01:31<1:36:25,  5.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000, avg_f1_score: 0.0231, avg_recall: 0.2833, avg_precision: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|‚ñè         | 19/1000 [02:00<1:36:24,  5.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000, avg_train_loss: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|‚ñè         | 19/1000 [02:02<1:36:24,  5.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000, avg_f1_score: 0.0246, avg_recall: 0.3030, avg_precision: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|‚ñè         | 24/1000 [02:32<1:36:38,  5.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000, avg_train_loss: 0.8386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|‚ñè         | 24/1000 [02:33<1:36:38,  5.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000, avg_f1_score: 0.0297, avg_recall: 0.3121, avg_precision: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|‚ñé         | 29/1000 [03:02<1:35:17,  5.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000, avg_train_loss: 0.8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|‚ñé         | 29/1000 [03:04<1:35:17,  5.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000, avg_f1_score: 0.0387, avg_recall: 0.3556, avg_precision: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|‚ñé         | 34/1000 [03:34<1:35:13,  5.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000, avg_train_loss: 0.7773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|‚ñé         | 34/1000 [03:36<1:35:13,  5.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000, avg_f1_score: 0.0510, avg_recall: 0.4518, avg_precision: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|‚ñç         | 39/1000 [04:04<1:35:09,  5.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000, avg_train_loss: 0.7452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|‚ñç         | 39/1000 [04:06<1:35:09,  5.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000, avg_f1_score: 0.0606, avg_recall: 0.4608, avg_precision: 0.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|‚ñç         | 44/1000 [04:35<1:32:21,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000, avg_train_loss: 0.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|‚ñç         | 44/1000 [04:37<1:32:21,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000, avg_f1_score: 0.0736, avg_recall: 0.4584, avg_precision: 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|‚ñç         | 48/1000 [04:54<1:32:58,  5.86s/epoch]"
     ]
    }
   ],
   "source": [
    "# Training Parameters and Frequency to Evaluate Validation Dataset\n",
    "num_epochs = 1000          # Number of epochs to train\n",
    "val_interval = 10          # Number of epochs between validation\n",
    "\n",
    "# Metrics for Saving Checkings \n",
    "# Options: (fBetaN, avg_{metric}, or {metric}_class{N} \n",
    "# where metric = [recall, precision, f1 or fBetaN]) \n",
    "best_metric = 'fBeta3'  \n",
    "\n",
    "results = model_trainer.train(\n",
    "    data_generator, model_save_path, max_epochs=num_epochs,\n",
    "    crop_size=model_config['dim_in'], my_num_samples=num_tomo_crops,\n",
    "    val_interval=val_interval, best_metric=best_metric, verbose=True\n",
    ")\n",
    "\n",
    "# Save parameters and results\n",
    "parameters_save_name = os.path.join(model_save_path, \"training_parameters.yaml\")\n",
    "io.save_parameters_to_yaml(model_builder, model_trainer, data_generator, parameters_save_name)\n",
    "\n",
    "results_save_name = os.path.join(model_save_path, \"results.json\")\n",
    "io.save_results_to_json(results, results_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîÅ (Optional): Use Optuna / Bayesian Optimization for Automatic Network Exploration\n",
    "\n",
    "In this optional step, we use Optuna, a Bayesian optimization framework, to automatically explore different network architectures and training hyperparameters. This process helps identify high-performing configurations without the need for exhaustive manual tuning.\n",
    "\n",
    "By leveraging intelligent sampling strategies, Optuna can efficiently search through:\n",
    "\n",
    "\t‚Ä¢\tNetwork depth and width (e.g., number of layers, channels)\n",
    "\t‚Ä¢\tLearning rates, dropout rates, and other optimization parameters\n",
    "\t‚Ä¢\tLoss function weights (e.g., Focal vs Tversky balance)\n",
    "\t‚Ä¢\tData sampling or augmentation strategies\n",
    "\n",
    "This automated search is especially useful when working with new biological targets with unknown optimal network setups.\n",
    "\n",
    "To run the model search outside this notebook, you can use the CLI:\n",
    "```\n",
    "octopi model-explore --help\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with:\n",
      "  ../config.json\n",
      "\n",
      "Number of training samples: 5\n",
      "Number of validation samples: 2\n",
      "Number of test samples: 0\n",
      "All training samples fit in memory. No reloading required.\n"
     ]
    }
   ],
   "source": [
    "from octopi.pytorch.model_search_submitter import ModelSearchSubmit\n",
    "\n",
    "#########################Input Parameters#########################\n",
    "\n",
    "# Target Parameters\n",
    "config = \"../config.json\"\n",
    "target_name = 'targets'\n",
    "target_user_id = 'octopi'\n",
    "target_session_id = None\n",
    "tomo_algorithm = 'wbp-denoised-denoiset-ctfdeconv'\n",
    "voxel_size = 10.012\n",
    "Nclass = 7\n",
    "\n",
    "# Define the model type\n",
    "model_type = \"Unet\"\n",
    "\n",
    "# Training and Exploration Parameters\n",
    "num_trials = 100\n",
    "num_epochs = 1000\n",
    "tomo_batch_size = 25\n",
    "best_metric = 'fBeta3'\n",
    "val_interval = 10\n",
    "\n",
    "# MLFlow Experiment Name\n",
    "mlflow_experiment_name = \"model_search\"\n",
    "\n",
    "# Random Seed\n",
    "random_seed = 42\n",
    "\n",
    "# Define the train and validate run IDs\n",
    "trainRunIDs = None\n",
    "validateRunIDs = None\n",
    "\n",
    "# Initialize the ModelSearchSubmit class\n",
    "search = ModelSearchSubmit(\n",
    "    config,\n",
    "    target_name, target_user_id, target_session_id,\n",
    "    tomo_algorithm, voxel_size, Nclass, model_type,\n",
    "    mlflow_experiment_name, random_seed, \n",
    "    num_epochs, num_trials, tomo_batch_size, best_metric, val_interval,\n",
    "    trainRunIDs, validateRunIDs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Search Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.run_model_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
